\documentclass[11pt,landscape,a4paper,fleqn]{article}
\usepackage[utf8]{inputenc}
\usepackage[british]{babel}
\usepackage{tikz}
\usetikzlibrary{shapes,positioning,arrows,fit,calc,graphs,graphs.standard}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage[top=4mm,bottom=5mm,left=4mm,right=4mm]{geometry}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{microtype}
\usepackage{mathtools}
\usepackage{ccicons}
\usepackage{hyperref}
\usepackage{lmodern}
\PassOptionsToPackage{dvipsnames}{xcolor}
\usepackage[dvipsnames]{xcolor}
\usepackage{soul}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[neveradjust]{paralist}
\usepackage[shortlabels]{enumitem}
\usepackage{bbm}
\usepackage{listings}
\usepackage{libertine}
\usepackage[libertine]{newtxmath}
% \usepackage{ETbb}
% \usepackage[sc]{mathpazo}
\usepackage{algpseudocode}
\usepackage{inconsolata}
\usepackage{physics}
\usepackage{bm}
\usepackage{svg}
\usepackage{wrapfig}
\usepackage{titlesec}
\usepackage{booktabs}

\providetoggle{showextratext}
\settoggle{showextratext}{false}

\setlength{\columnsep}{2mm}

\setlist{topsep=0pt, leftmargin=*, noitemsep, topsep=0pt,parsep=0pt,partopsep=0pt}

\newcommand{\extratext}[1]{\iftoggle{showextratext}{#1}{}}

\newcommand{\todo}[1]{\textcolor{red}{\textbf{TODO:} #1}}

\newcommand*{\tran}{^{\mathsf{T}}} % (DIN) EN ISO 80000-2:2013
\newcommand{\kl}[2]{D_{\mathrm{KL}}(#1\lVert#2)}
\newcommand{\js}[2]{D_{\mathrm{JS}}(#1\lVert#2)}

\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator{\softmax}{\mathrm{Softmax}}
\DeclareMathOperator{\diag}{\mathrm{diag}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\exp}{\mathrm{exp}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\sep}{\,\textcolor{red}{\bm|}\,}

\setdefaultleftmargin{0.5cm}{}{}{}{}{}

\let\bar\overline

\definecolor{myblue}{cmyk}{1,.72,0,.38}
\definecolor{myorange}{cmyk}{0,0.5,1,0}
\definecolor{myorange2}{cmyk}{0,0.8,0.8,0}
\definecolor{darkgreen}{cmyk}{0.97,0,1,0.57}

\pgfdeclarelayer{background}
\pgfsetlayers{background,main}

%\everymath\expandafter{\the\everymath \color{myblue}}
%\everydisplay\expandafter{\the\everydisplay \color{myblue}}

\renewcommand{\baselinestretch}{.8}
\pagestyle{empty}

\setlength{\parindent}{0pt}

\def\myvector#1{\mathbf{#1}}
\def\va{{\myvector{a}}}
\def\vb{{\myvector{b}}}
\def\vc{{\myvector{c}}}
\def\vd{{\myvector{d}}}
\def\ve{{\myvector{e}}}
\def\vf{{\myvector{f}}}
\def\vg{{\myvector{g}}}
\def\vh{{\myvector{h}}}
\def\vi{{\myvector{i}}}
\def\vj{{\myvector{j}}}
\def\vk{{\myvector{k}}}
\def\vl{{\myvector{l}}}
\def\vm{{\myvector{m}}}
\def\vn{{\myvector{n}}}
\def\vo{{\myvector{o}}}
\def\vp{{\myvector{p}}}
\def\vq{{\myvector{q}}}
\def\vr{{\myvector{r}}}
\def\vs{{\myvector{s}}}
\def\vt{{\myvector{t}}}
\def\vu{{\myvector{u}}}
\def\vv{{\myvector{v}}}
\def\vw{{\myvector{w}}}
\def\vx{{\myvector{x}}}
\def\vy{{\myvector{y}}}
\def\vz{{\myvector{z}}}

\def\mymatrix#1{\mathbf{#1}}
\def\mA{{\mymatrix{A}}}
\def\mB{{\mymatrix{B}}}
\def\mC{{\mymatrix{C}}}
\def\mD{{\mymatrix{D}}}
\def\mE{{\mymatrix{E}}}
\def\mF{{\mymatrix{F}}}
\def\mG{{\mymatrix{G}}}
\def\mH{{\mymatrix{H}}}
\def\mI{{\mymatrix{I}}}
\def\mJ{{\mymatrix{J}}}
\def\mK{{\mymatrix{K}}}
\def\mL{{\mymatrix{L}}}
\def\mM{{\mymatrix{M}}}
\def\mN{{\mymatrix{N}}}
\def\mO{{\mymatrix{O}}}
\def\mP{{\mymatrix{P}}}
\def\mQ{{\mymatrix{Q}}}
\def\mR{{\mymatrix{R}}}
\def\mS{{\mymatrix{S}}}
\def\mT{{\mymatrix{T}}}
\def\mU{{\mymatrix{U}}}
\def\mV{{\mymatrix{V}}}
\def\mW{{\mymatrix{W}}}
\def\mX{{\mymatrix{X}}}
\def\mY{{\mymatrix{Y}}}
\def\mZ{{\mymatrix{Z}}}

\titleformat{\section}{\color{myorange}\sffamily\normalsize}{\thesection}{0.5em}{}
\titleformat{\subsection}[runin]{\color{myorange2}\sffamily\small}{\thesubsection}{0.5em}{}
\titleformat{\subsubsection}[runin]{\color{violet}\sffamily\small}{\thesubsubsection}{0.5em}{}

\titlespacing*{\section}
{0pt}{0.3em}{0em}
\titlespacing*{\subsection}
{0pt}{0.3em}{1em}
\titlespacing*{\subsubsection}
{0pt}{0.1em}{1em}

\begin{document}
\setlength{\columnseprule}{0.1pt}
	
% \section*{Disclaimer}

\begin{multicols*}{4}
\setlength{\columnseprule}{0.0pt}

This document is a summary of the \textit{Machine Perception} course at ETH Z\"urich.
This summary was created during the spring semester of 2024.
Due to updates to the syllabus content, some material may no longer be relevant for future versions of the course.
I do not guarantee correctness or completeness, nor is this document endorsed by the lecturers.
The order of the chapters is not necessarily the order in which they were presented in the course.
For the full \LaTeX \ source code, visit \texttt{\href{https://github.com/Jovvik/eth-cheatsheets}{github.com/Jovvik/eth-cheatsheets}}.

All figures are created by the author, and, assuming the rules have not been changed,
are not allowed to be used as a part of a cheat sheet during the exam.

\newpage
\setlength{\columnseprule}{0.1pt}
\section{CNN} $T$ is linear if $T(\alpha \vu + \beta \vv) = \alpha T(\vu) + \beta T(\vv)$,
invariant to $f$ if $T(f(\vu)) = T(\vu)$,
equivariant to $f$ if $T(f(\vu)) = f(T(\vu))$.
Any linear shift-equivariant $T$ can be written as a convolution.
Convolution:
$I'(i, j) = \sum_{m =- k}^k \sum_{n =- k}^k K(\textcolor{red}{-}m, \textcolor{red}{-}n)I(m + i, n + j)$.
Correlation:
$I'(i, j) = \sum_{m =- k}^k \sum_{n =- k}^k K(m, n)I(m + i, n + j)$.
Conv. forward: $z^{(l)} = w^{(l)} * z^{(l - 1)} + b^{(l)} = \sum_m \sum_n w_{m, n}^{(l)} z_{i - m, j - n}^{(l - 1)} + b^{(l)}$.
Backward inputs: $\delta^{(l - 1)} = \pdv{C}{z_{i,j}^{(l - 1)}} = \delta^{(l)} * \mathrm{ROT}_{180}(w^{(l)})$,
backward kernel: $\pdv{C}{w_{m, n}^{(l)}} = \delta^{(l)} * \mathrm{ROT}_{180}(z^{(l - 1)})$.
Width or height after conv or pool: $(\mathrm{in} + 2 \cdot \mathrm{pad} - \mathrm{dil} \cdot (\mathrm{kern} - 1) - 1) / \mathrm{stride} + 1$,
rounded down.
Channels = number of kernels.

1D conv as matmul:
\(\begin{bmatrix}
    k_1 & 0 & \dots & 0 \\
    k_2 & k_1 &  & \vdots \\
    k_3 & k_2 & k_1 & 0 \\
    0 & k_3 & k_2 & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
\end{bmatrix}\)

Backprop example (\textcolor{red}{rotate $\mK$}):
\[
    \stackrel{\begin{bmatrix}
        0 & 0 & 1 & 1 & 0 \\
        0 & 1 & 0 & 1 & 1 \\
        1 & 1 & 1 & 1 & 0 \\
        1 & 0 & 1 & 1 & 0 \\
        1 & 1 & 0 & 1 & 1
    \end{bmatrix}}{\mX} \to
    \stackrel{\begin{bmatrix}
        1 & 0 & 1 \\
        0 & 1 & 0 \\
        0 & 1 & 0 
    \end{bmatrix}}{\mK} \to
    \stackrel{\begin{bmatrix}
        3 & 3 & 3 \\
        4 & 2 & 3 \\
        2 & 3 & 3 
    \end{bmatrix}}{\mY = \mX * \mK} \to
\]
\[
    \to\stackrel{\begin{bmatrix}
        4
    \end{bmatrix}}{\mY' = \mathrm{Pool}(\mY)}
    \sep
    \stackrel{\begin{bmatrix}
        1
    \end{bmatrix}}{\partial E / \partial \mY'} \to
    \stackrel{\begin{bmatrix}
        0 & 0 & 0 \\
        1 & 0 & 0 \\
        0 & 0 & 0 \\
    \end{bmatrix}}{\partial E / \partial \mY} \to
\]
\[
    \to\stackrel{\begin{bmatrix}
        1 & 0 & 1 \\
        1 & 1 & 1 \\
        0 & 1 & 0 \\
    \end{bmatrix}}{\partial E / \partial \mK} \to
    \stackrel{\begin{bmatrix}
        0 & 0 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 & 0 \\
        1 & 0 & 1 & 0 & 0 \\
        0 & 0 & 0 & 0 & 0
    \end{bmatrix}}{\partial E / \partial \mV}
\]

Max-pooling: $z^{(l)} = \max z_i^{(l - 1)}$. $i^* \coloneqq \argmax_i z_i^{(l - 1)}$,
$\pdv{z^{(l)}}{z_i^{(l - 1)}} = [i = i^*]$, $\delta^{(l - 1)} = \delta^{(l)}_{i^*}$.
Unpooling: nearest-neighbor (duplicate), bed of nails (only top left, rest 0), max-unpooling (remember where max came from when pooling).
Learnable upsampling: transposed conv, output is copies of filter weighted by input, summed on overlaps.

\section{RNN}

\textbf{Vanilla \textsc{rnn}}: $\hat{y}_t = \mW_{hy} \vh_t, \vh_t = f(\vh_{t-1}, \vx_t, \mW)$
, usually $\vh_t = \tanh(\mW_{hh} \vh_{t-1} + \mW_{xh}\vx_t)$.

\textbf{BPTT}: $\pdv{L}{\mW} = \sum_t \pdv{L_t}{\mW}$, treat unrolled model as multi-layer.
$\pdv{L_t}{W}$ has a term of $\pdv{\vh_t}{\vh_k} = \prod_{i = k + 1}^t \pdv{\vh_i}{\vh_{i-1}}
= \prod_{i = k + 1}^t \mW_{hh}\tran \diag f'(\vh_{i-1})$.

\textbf{Exploding/vanishing gradients}: $\vh_t = \mW^t \vh_1$.
If $\mW$ is diagonaliz., $\mW = \mQ \diag \bm\lambda \mQ\tran = \mQ \bm\Lambda \mQ\tran, \mQ \mQ\tran = \mI
\Rightarrow \vh_t = (\mQ\bm\Lambda \mQ\tran)^t \vh_1 = (\mQ (\diag \bm\lambda)^t \mQ\tran) \vh_1$
$\Rightarrow \vh_t$ becomes the dominant eigenvector of $\mW$.
$\pdv{\vh_t}{\vh_k}$ has this issue.
Long-term contributions vanish, too sensitive to recent distrations.
\textbf{Truncated BPTT}: take the sum only over the last $\kappa$ steps.
\textbf{Gradient clipping} $\frac{\mathrm{threshold}}{\norm{\nabla}} \nabla$ fights exploding gradients.


\subsection{LSTM}
We want constant error flow, not multiplied by $W^t$.
\begin{itemize}
    \item Input gate: which values to write,
    \item forget gate: which values to reset,
    \item output gate: which values to read,
    \item gate: candidate values to write to state.
\end{itemize}

\begin{minipage}{0.3\linewidth}
    \begin{align*}
        \begin{pmatrix}
            \vi \\
            \vf \\
            \vo \\
            \vg
        \end{pmatrix} & = \begin{pmatrix}
            \sigma \\
            \sigma \\
            \sigma \\
            \tanh
        \end{pmatrix} W \begin{pmatrix}
            \vx_t \\
            \vh_{t-1}
        \end{pmatrix} \\
        \vc_t & = \vf \odot \vc_{t-1} + \vi \odot \vg \\
        \vh_t & = \vo \odot \tanh(\vc_t)
    \end{align*}
\end{minipage}%
% \begin{minipage}{0.4\linewidth}
%     \includesvg[width=\textwidth]{figures/LSTM.svg}
% \end{minipage}


\section{Generative modelling}

Learn $p_{\mathrm{model}} \approx p_{\mathrm{data}}$, sample from $p_{\mathrm{model}}$.

\begin{itemize}[leftmargin=0.5em]
    \item Explicit density:
    \begin{itemize}[leftmargin=0.3em]
        \item Approximate:
        \begin{itemize}[leftmargin=0.0em]
            \item Variational: VAE, Diffusion
            \item \textcolor{gray}{Markov Chain: Boltzmann machine}
        \end{itemize}
        \item Tractable:
        \begin{itemize}[leftmargin=0.0em]
            \item Autoregressive: FVSBN/NADE/MADE, Pixel\textsc{(c/r)nn}, WaveNet/\textsc{tcn}, Autor. Transf., 
            \item Normalizing Flows
        \end{itemize}
    \end{itemize}
    \item Implicit density:
    \begin{itemize}[leftmargin=0.3em]
        \item Direct: Generative Adversarial Networks
        \item \textcolor{gray}{MC: Generative Stochastic Networks}
    \end{itemize}
\end{itemize}

Autoencoder: $X \textcolor{blue}{\to} Z \textcolor{red}{\to} X$, $\textcolor{red}{g} \circ \textcolor{blue}{f} \approx \mathrm{id}$,
$f$ and $g$ are NNs. Optimal linear autoencoder is PCA.

Undercomplete: $|Z| < |X|$, else overcomplete.
Overcomp. is for denoising, inpainting.

Latent space should be continuious and interpolable.
Autoencoder spaces are neither,
so they are only good for reconstruction.

\section{Variational AutoEncoder (VAE)}

Sample $z$ from prior $p_\theta(z)$, to decode use conditional $p_\theta(x \mid z)$ defined by a NN.

$\kl{P}{Q} \coloneqq \int_x p(x) \log \frac{p(x)}{q(x)} \dd x$: KL divergence,
measure similarity of prob. distr.

$\kl{P}{Q} \neq \kl{Q}{P}, \kl{P}{Q} \geq 0$

$z$ can also be categorical.
Likelihood $p_\theta(x) = \int_z p_\theta(x \mid z) p_\theta(z) \dd z$ is hard to maximize,
let encoder NN be $q_\phi(z \mid x)$,
$\log p_\theta(x^{i}) = \textcolor{orange}{\E_z\left[\log p_\theta(x^{i} \mid z)\right]}
- \textcolor{purple}{\kl{q_\phi(z \mid x^{i})}{p_\theta(z)}} + \textcolor{red}{\kl{q_\phi(z \mid x^{i})}{p_\theta(z \mid x^{i})}}$.
\textcolor{red}{Red} is intractable, use $\geq 0$ to ignore it;
\textcolor{orange}{Orange} is reconstruction loss, clusters similar samples;
\textcolor{purple}{Purple} makes posterior close to prior, adds cont. and interp.
$\mathrm{\textcolor{orange}{Orange}} - \mathrm{\textcolor{purple}{Purple}}$ is \textbf{ELBO}, maximize it.

$x \xrightarrow{\mathrm{\makebox[0pt]{\scriptsize \textcolor{blue}{enc}}}} \mu_{z \mid x}, \Sigma_{z \mid x} \xrightarrow{\mathrm{sample}} z \xrightarrow{\mathrm{\makebox[0pt]{\scriptsize \textcolor{red}{dec}}}} \mu_{x \mid z}, \Sigma_{x \mid z}  \xrightarrow{\mathrm{sample}} \hat{x}$

Backprop through sample by reparametr.: $z = \mu + \sigma \epsilon$.
For inference, use $\mu$ directly.

Disentanglement: features should correspond to distinct factors of variation.
Can be done with semi-supervised learning by making $z$ conditionally independent of given features $y$.

\subsection{$\beta$-VAE}

$\max_{\theta, \phi} \E_x\left[\E_{z \sim q_\phi} \log p_\theta(x \mid z)\right]$
to disentangle s.t.
$\kl{q_\phi(z \mid x)}{p_\theta(z)} < \delta$, with KKT: $\max \textcolor{orange}{\mathrm{Orange}} - \beta\textcolor{purple}{\mathrm{Purple}}$.

\section{Autoregressive generative models}

Autoregression: use data from the same input variable at previous time steps

Discriminative: $P(Y \mid X)$, generative: $P(X, Y)$, maybe with $Y$ missing.
Sequence models are generative: from $x_i \dots x_{i + k}$ predict $x_{i + k + 1}$.

Tabular approach: $p(\vx) = \prod_i p(x_i \mid \vx_{<i})$, needs $2^{i - 1}$ params.
Independence assumption is too strong.
Let $p_{\theta_i}(x_i \mid \vx_{ < i}) = \operatorname{Bern}(f_i(\vx_{ < i}))$,
where $f_i$ is a NN.
\textbf{Fully Visible Sigmoid Belief Networks}: $f_i = \sigma(\alpha^{(i)}_0 + \bm{\alpha}^{(i)} \vx_{ < i}\tran)$,
complexity $n^2$, but model is linear.

\textbf{Neural Autoregressive Density Estimator}: add hidden layer.
$\vh_i = \sigma(\vb + \mW_{\centerdot, < i} \vx_{ < i})$,
$\hat{x}_i = \sigma(c_i + \mV_{i,\centerdot} \vh_i)$.
Order of $\vx$ can be arbitrary but fixed.
Train by max log-likelihood in $\mathcal{O}(TD)$, can use 2nd order optimizers,
can use \textbf{teacher forcing}: feed GT as previous output.

Extensions: Convolutional; Real-valued: conditionals by mixture of gaussians;
Order-less and deep: one DNN predicts $p(x_k \mid x_{i_1} \dots x_{i_j})$.

\textbf{Masked Autoencoder Distribution Estimator}:
mask out weights s.t. no information flows from $x_d \dots $ to $\hat{x}_d$.
Large hidden layers needed.
Trains as fast as autoencoders, but sampling needs $D$ forward passes.

\textbf{PixelRNN}: generate pixels from corner, dependency on previous pixels is by RNN (LSTM).
\textbf{PixelCNN}: also from corner, but condition by CNN over context region (perceptive field) $ \Rightarrow $ parallelize.
For conditionals use masked convolutions.
Channels: model R from context, G from R + cont., B from G + R + cont.
Training is parallel, but inference is sequential $ \Rightarrow $ slow.
Use conv. stacks to mask correctly.

NLL is a natural metric for autoreg. models,
hard to evaluate others.

\textbf{WaveNet}: audio is high-dimensional.
Use dilated convolutions to increase perceptive field with multiple layers.

AR does not work for high res images/video, convert the images into a series of tokens with an AE:
Vector-quantized VAE.
The codebook is a set of vectors.
$x \xrightarrow{\mathrm{\makebox[0pt]{\scriptsize \textcolor{blue}{enc}}}} z \xrightarrow{\mathrm{codebook}} z_q \xrightarrow{\mathrm{\makebox[0pt]{\scriptsize \textcolor{red}{dec}}}} \hat{x}$.

We can run an AR model in the latent space.

\subsection{Attention}

$\vx_t$ is a convex combination of the past steps, with access to all past steps.
For $X \in \R^{T \times D}$: 
$K = XW_K, V = XW_V, Q = XW_Q$.
Check pairwise similarity between query and keys via dot product:
let attention weights be $\bm{\alpha} = \softmax(QK\tran / \sqrt{D})$, $\bm{\alpha} \in \R^{1 \times T}$.
Adding mask $M$ to avoid looking into the future:
\[X = \softmax\left(\frac{(XW_Q)(XW_K)\tran}{\sqrt{D}} + M\right)(XW_V)\]
Multi-head attn. splits $W$ into $h$ heads, then concatenates them.
Positional encoding injects information about the position of the token.
Attn. is $\mathcal{O}(T^2 D)$.

\section{Normalizing Flows}

VAs dont have a tractable likelihood, AR models have no latent space.
Want both.
Change of variable for $x = f(z)$:
$p_x(x) = p_z(f^{-1}(x)) \abs{\det \pdv{f^{-1}(x)}{x}} = p_z(f^{-1}(x)) \abs{\det \pdv{f(z)}{z}}^{-1}$.
Map $Z \to X$ with a deterministic invertible $f_\theta$.
This can be a NN, but computing the determinant is $\mathcal{O}(n^3)$.
If the Jacobian is triangular, the determinant is $\mathcal{O}(n)$.
To do this, add a coupling layer:
\begin{minipage}{0.5\linewidth}
    \[\begin{pmatrix}
        y^A \\
        y^B
    \end{pmatrix} = \begin{pmatrix}
        h(x^A, \beta(x^B)) \\
        x^B
    \end{pmatrix}\]
\end{minipage}%
\hspace{0.3cm}%
\begin{minipage}{0.45\linewidth}
    , where $\beta$ is any model, and $h$ is elementwise.
\end{minipage}
\[\begin{pmatrix}
    x^A \\
    x^B
\end{pmatrix} = \begin{pmatrix}
    h^{-1}(y^A, \beta(y^B)) \\
    y^B
\end{pmatrix}, J = \begin{pmatrix}
    h' & h'\beta' \\
    0 & 1
\end{pmatrix}\]
Stack these for expressivity, $f = f_k \circ \dots f_1$.
$p_x(x) = p_z(f^{-1}(x)) \prod_k \abs{\det \pdv{f_k^{-1}(x)}{x}}$.

Sample $z \sim p_z$ and get $x = f(z)$.

% \includesvg[width=\linewidth]{figures/flow.svg}

% \begin{wrapfigure}{R}{0.2\linewidth}
%     \includesvg[width=\linewidth]{figures/flow-block.svg}
% \end{wrapfigure}
\textbullet\ Squeeze: reshape, increase chan.

\textbullet\ ActNorm: batchnorm with init. s.t. output $\sim \mathcal{N}(0, \mI)$ for first minibatch.
    $\vy_{i,j} = \vs \odot \vx_{i,j} + \vb$,
    $\vx_{i, j} = (\vy_{i,j} - \vb) / \vs$,
    $\log\det = H \cdot W \cdot \sum_i \log \abs{\vs_i}$: linear.

\textbullet\ $1 \times 1$ conv: permutation along channel dim.
    Init $\mW$ as rand. ortogonal $\in \R^{C \times C}$ with $\det\mW = 1$.
    $\log\det = H \cdot W \cdot \log\abs{\det\mW}$: $\mathcal{O}(C^3)$.
    Faster: $\mW \coloneqq \mP \mL(\mU + \diag(s))$,
    where $\mP$ is a random \underline{fixed} permut. matrix,
    $\mL$ is lower triang. with 1s on diag.,
    $\mU$ is upper triang. with 0s on diag.,
    $\vs$ is a vector.
    Then $\log\det = \sum_i \log \abs{\vs_i}$: $\mathcal{O}(C)$
Conditional coupling: add parameter $\vw$ to $\beta$.

\textbf{SRFlow}: use flows to generate many high-res images from a low-res one.
Adds affine injector between conv. and coupling layers.
$\vh^{n+1} = \exp(\beta^n_{\theta, s}(\vu)) \cdot \vh^n + \beta_{\theta, b}(\vu)$,
$\vh^n = \exp( - \beta^n_{\theta, s}(\vu)) \cdot (\vh^{n+1} - \beta^n_{\theta, b}(\vu))$,
$\log\det = \sum_{i,j,k} \beta^n_{\theta, s}(\vu_{i, j, k})$.

\textbf{StyleFlow}: Take StyleGAN and replace the network $\vz \to \vw$ (aux. latent space)
with a normalizing flow conditioned on attributes.

\textbf{C-Flow}: condition on other normalizing flows: multimodal flows.
Encode original image $\vx_B^1$: $\vz_B^1 = f^{-1}_\phi(\vx_B^1 \mid \vx_A^1)$;
encode extra info (image, segm. map, etc.) $\vx_A^2$: $\vz_A^2 = g^{-1}_\theta(\vx_A^2)$;
generate new image $\vx_B^2$: $\vx_B^2 = f_\phi(\vz_B^1 \mid \vz_A^2)$.

Flows are expensive for training and low res.
The latent distr. of a flow needn't be $\mathcal{N}$.

\section{Generative Adversarial Networks (GANs)}

Log-likelihood is not a good metric. We can have high likelihood with poor quality by mixing in noise and not losing much likelihood; or low likelihood with good quality by remembering input data and having sharp peaks there.

\textbf{Generator} $G : \R^Q \to \R^D$ maps noise $z$ to data,
\textbf{discriminator} $D : \R^D \to [0, 1]$ tries to decide if data is real or fake,
receiving both gen. outputs and training data.
Train $D$ for $k$ steps for each step of $G$.

Training GANs is a min-max process, which are hard to optimize.
$V(G, D) = \E_{\vx \sim p_{\mathrm{d}}} \log(D(\vx)) + \E_{\hat{\vx} \sim p_{\mathrm{m}}} \log(1 - D(\hat{\vx}))$

For $G$ the opt. $D^* = p_{\mathrm{d}}(\vx) / (p_{\mathrm{d}}(\vx) + p_{\mathrm{m}}(\vx))$.

Jensen-Shannon divergence (symmetric):
$\js{p}{q} = \frac{1}{2} \kl{p}{\frac{p + q}{2}} + \frac{1}{2} \kl{p}{\frac{p + q}{2}}$.
Global minimum of $\js{p_{\mathrm{d}}}{p_{\mathrm{m}}}$ is the glob. min. of $V(G, D)$,
$V(G, D^*) = - \log(4)$ and at optimum of $V(D^*, G)$ we have $p_d = p_m$.

If $G$ and $D$ have enough capacity, at each update step $D$ reaches $D^*$
and $p_{\mathrm{m}}$ improves $V(p_{\mathrm{m}}, D^*) \propto \sup_D \int_{\vx} p_{\mathrm{m}}(\vx) \log( - D(\vx)) \dd \vx$,
then $p_{\mathrm{m}} \to p_{\mathrm{d}}$ by convexity of $V(p_{\mathrm{m}}, D^*)$ wrt. $p_{\mathrm{m}}$.
These assumptions are too strong.

If $D$ is too strong, $G$ has near zero gradients and doesn't learn ($\log'(1 - D(G(z))) \approx 0$).
Use gradient \underline{ascent} on $\log(D(G(z)))$ instead.

Model collapse: $G$ only produces one sample or one class of samples.
Solution: \textbf{unrolling} --- use $k$ previous $D$ for each $G$ update.

GANs are hard to compare, as likelihood is intractable.
FID is a metric that calculates the distance between feature vectors
calculated for real and generated images.

DCGAN: pool $\to$ strided convolution, batchnorm, no FC, ReLU for $G$, LeakyReLU for $D$.

Wasserstein GAN: different loss, gradients don't vanish.
Adding gradient penalty for $D$ stabilizes training.
Hierarchical GAN: generate low-res image, then high-res during training.
StyleGAN: learn intermediate latent space $\mathcal{W}$ with FCs,
batchnorm with scale and mean from $\mathcal{W}$, add noise at each layer.

GAN \textbf{inversion}: find $z$ s.t. $G(z) \approx x$ $ \Rightarrow $ manipulate images in latent space, inpainting.
If $G$ predicts image and segmentation mask,
we can use inversion to predict mask for any image, even outside the training distribution.

\subsection{3D GANs}

3D GAN: voxels instead of pixels.
PlatonicGAN: 2D input, 3D output differentiably rendered back to 2D for $D$.

HoloGAN: 3D GAN + 2D superresolution GAN

GRAF: radiance fields more effic. than voxels

GIRAFFE: GRAF + 2D conv. upscale

EG3D: use 3 2D images from StyleGAN for features, project each 3D point to tri-planes.

\subsection{Image Translation}

E.g. sketch $X \to$ image $Y$.
Pix2Pix:
$G : X \to Y$,
$D : X, Y \to [0, 1]$.
GAN loss $+ L_1$ loss between sketch and image.
Needs pairs for training.

CycleGAN: unpaired.
Two GANs $F: X \to Y, G : Y \to X$,
cycle--consistency loss $F \circ G \approx \mathrm{id}; G \circ F \approx \mathrm{id}$
plus GAN losses for $F$ and $G$.

BicycleGAN: add noise input.

Vid2vid: video translation.

\section{Diffusion models}

High quality generations, better diversity, more stable/scalable.

Diffusion (forward) step $q$: adds noise to $\vx_t$ (not learned).
Denoising (reverse) step $p_\theta$: removes noise from $\vx_t$ (learned).

$q(\vx_t \mid \vx_{t-1}) = \mathcal{N}(\sqrt{1 - \beta} \vx_{t-1}, \beta_t \mI)$

$p_\theta(\vx_{t-1} \mid \vx_t) = \mathcal{N}(\mu_\theta(\vx_t, t), \sigma_t^2 \mI)$

$\beta_t$ is the variance schedule (monotone $\uparrow$).
Let $\alpha_t \coloneqq 1 - \beta_t, \overline{\alpha}_t \coloneqq \prod \alpha_i$,
then $q(\vx_t \mid \vx_0) = \mathcal{N}(\sqrt{\overline{\alpha}_t} \vx_0, (1 - \overline{\alpha}_t)\mI)
\Rightarrow \vx_t = \sqrt{\overline{\alpha}_t} \vx_0 + \sqrt{1 - \overline{\alpha}_t}\epsilon$.

Denoising is not tractable naively:
$q(\vx_{t-1} \mid \vx_t) = q(\vx_t \mid \vx_{t-1}) q(\vx_{t-1}) / q(\vx_t)$,
$q(\vx_t) = \int q(\vx_t \mid \vx_0) q(\vx_0) \dd \vx_0$.

Conditioning on $\vx_0$ we get a Gaussian.
Learn model $p_\theta(\vx_{t-1} \mid \vx_t) \approx q(\vx_{t-1} \mid \vx_t, \vx_0)$
by predicting the mean.

$\log p(\vx_0) \geq \E_{q(\vx_{1:T} \mid \vx_0)} \log(\frac{p(\vx_{0:T})}{q(\vx_{1:T} \mid \vx_0)}) =
\textcolor{orange}{\E_{q(\vx_1 \mid \vx_0)}\log p_\theta(\vx_0 \!\!\mid\!\! \vx_1)} -
\textcolor{purple}{\kl{q(\vx_T \!\!\mid\!\! \vx_0)}{p(\vx_T)}} -
\textcolor{blue}{\sum_{t = 2}^T \E_{q(\vx_t \mid \vx_0)} \kl{q(\vx_{t-1} \!\!\mid\!\! \vx_t, \vx_0)}{p_\theta(\vx_{t-1} \!\!\mid\!\! \vx_t)}}$,
where \textcolor{orange}{orange} and \textcolor{purple}{purple} are the same as in VAEs,
and \textcolor{blue}{blue} are the extra loss functions.
In a sense VAEs are 1-step diffusion models.

$t$-th denoising is just $\argmin_\theta \frac{1}{2 \sigma_q^2(t)} \norm{\mu_\theta - \mu_q}_2^2$,
so we want $\mu_\theta(\vx_t, t) \approx \mu_q(\vx_t, \vx_0)$.
$\mu_q(\vx_t, \vx_0)$ can be written as
$\frac{1}{\sqrt{\alpha_t}} \vx_t - \frac{1 - \alpha_t}{\sqrt{1 - \overline{\alpha}_t} \sqrt{\alpha_t}} \epsilon_0$,
and $\mu_\theta(\vx_t, t) = \frac{1}{\sqrt{\alpha_t}} \vx_t - \frac{1 - \alpha_t}{\sqrt{1 - \overline{\alpha}_t} \sqrt{\alpha_t}} \hat{\epsilon}_\theta(\vx_t, t)$,
so the NN learns to predict the added noise.

Training: img $\vx_0, t \sim \mathrm{Unif}(1... T), \epsilon \sim \mathcal{N}(0, \mI)$,
GD on $\nabla_\theta\norm{\epsilon - \epsilon_\theta(\sqrt{\overline{\alpha}_t}\vx_0 + \sqrt{1 - \overline{\alpha}_t}\epsilon, t)}^2$.

Sampling: $\vx_T \sim \mathcal{N}(0, \mI)$, for $t = T$ downto $1$:
$\vz \sim \mathcal{N}(0, I)$ if $t > 1$ else $\vz = 0$;

$\vx_{t-1} = \frac{1}{\sqrt{\alpha_t}}(\vx_t - \frac{1 - \alpha_t}{\sqrt{1 - \overline{\alpha}_t}} \epsilon_\theta(\vx_t, t)) + \sigma_t \vz$.

$\sigma_t^2 = \beta_t$ in practice.
$t$ can be continuious.

\subsection{Conditional generation}

Add input $y$ to the model.

\textbf{ControlNet}: don't retrain model, add layers that add something to block outputs.

(Classifier-free) \textbf{guidance}: mix predictions of a conditional and unconditional model,
because conditional models are not diverse.
$\eta_{\theta_1}(x, c; t) = (1 + \rho)\eta_{\theta_1}(x, c; t) - \rho \eta_{\theta_2}(x; t)$.

\subsection{Latent diffusion models}

High-res images are expensive to model.
Predict in latent space, decode with a decoder.

\section{Reinforcement learning}

Environment is a Markov Decision Process:
states $S$, actions $A$,
reward $r : S \times A \to \R$,
transition $p: S \times A \to S$,
initial $s_0 \in S$,
discount factor $\gamma$.
$r$ and $p$ are deterministic, can be a distribution.
Learn policy $\pi : S \to A$.
Value $V_\pi: S \to \R$, the reward from $s$ under $\pi$.
\textbf{Bellman eq.}: $G_t \coloneqq \sum_{k = 0}^\infty \gamma^k R_{t + k + 1},$
$v_\pi(s) \coloneqq \E_\pi[G_t \mid S_t = s]
= \E_\pi[R_{t+1} + \gamma G_{t+1} \mid S_t = s]
= \sum_a \pi(a \mid s) \sum_{s'} \sum_r p(s', r \mid s, a)[r + \gamma \E_\pi[G_{t+1} \mid S_{t+1} = s']]
= \sum_a \pi(a \mid s) \sum_{s', r} p(s', r \mid s, a)[r + \gamma v_\pi(s')]]$.
Can be solved via dynamic programming (needs knowledge of $p$),
Monte-Carlo or Temporal Difference learning.

\subsection{Dynamic programming}

Value iteration: compute optimal $v_*$, then $\pi_*$.

Policy iteration: compute $v_\pi$ and $\pi$ together.

For any $V_\pi$ the greedy policy (optimal) is
$\pi'(s) = \argmax_{a \in A}(r(s, a) + \gamma V_\gamma(p(s, a)))$.

\textbf{Bellman optimality}: $v_*(s) = \max_a q_*(s, a)
= \max_a \sum_{s', r} p(s', r \mid s, a)[r + \gamma v_*(s')]$
$ \Rightarrow $ update step:
$V_{\mathrm{new}}^*(s) = \max_{a \in A} (r(s, a) + \gamma V_{\mathrm{old}}^*(s'))$,
when $V_{\mathrm{old}}^* = V_{\mathrm{new}}^*$, we have optimal policy.

Converges in finite steps, more efficient than policy iteration.
But needs knowledge of $p$,
iterates over all states and $\mathcal{O}(|S|)$ memory.

\subsection{Monte Carlo sampling}

Sample trajectories, estimate $v_\pi$ by averaging returns.
Doesn't need full $p$, is unbiased,
but high variance, exploration/exploitation dilemma, may not reach term. state.

\subsection{Temporal Difference learning}

For each $s \to s'$ by action $a$ update:
$\Delta V(s) = r(s, a) + \gamma V(s') - V(s)$.
\textbf{$\bm\varepsilon$-greedy policy}: with prob. $\varepsilon$ choose random action, else greedy.

\subsection{Q-learning}

$Q$-value f.: $q_\pi(s, a) \coloneqq \E_\pi[G_t \mid S_t = s, A_t = a]$.

\textbf{SARSA} (on-policy):
For each $S \to S'$ by action $A$ update:
$\Delta Q(S, A) = r(S, A) + \gamma Q(S', A') - Q(S, A)$,
$Q(S, A) {}+{}\!\!\!= \alpha \Delta Q(S, A)$,
$\alpha$ is LR.

\textbf{Q-learning} (off-policy/offline):
$\Delta Q(S, A) = R_{t+1} + \gamma \max_a Q(S', a) - Q(S, A)$

All these approaches do not approximate values of states that have not been visited.

\subsection{Deep Q-learning}

Use NN to predict $Q$-values.
Loss is $(R + \gamma \max_{a'}Q_\theta(S', a') - Q_\theta(S, A))^2$,
backprop only through $Q_\theta(S, A)$.
Store history in replay buffer, sample from it for training $ \Rightarrow $ no correlation in samples.

\subsection{Deep Q-networks}

Encode state to low dimensionality with NN.

\subsection{Policy gradients}

$Q$-learning does not handle continuous action spaces.
Learn a policy directly instead,
$\pi(a_t \mid s_t) = \mathcal{N}(\mu_t, \sigma_t^2 \mid s_t)$.
Sample trajectories: $p(\tau) = p(s_1, a_1, \dots, s_T, a_T) = p(s_1) \prod \pi(a_t \!\!\mid\!\! s_t) p(s_{t+1} \!\!\mid\!\! a_t, s_t)$.
This is on-policy.

Eval: $J(\theta) \coloneqq \E_{\tau \sim p_\theta(\tau)} [\sum_t \gamma^t r(s_t, a_t)]$.
To optimize, need to compute $\E$ (see proofs).

\textbf{REINFORCE}: MC sampling of $\tau$.
To reduce variance, subtract baseline $b(s_t)$ from reward.

\subsection{Actor-Critic}

$\nabla_\theta J(\theta) = \frac{1}{N} \sum_i \sum_t \nabla \log \pi_\theta(a_t^i \mid s_t^i)(r(s_t^i, a_t^i) + \gamma V(s_{t+1}^i) - V(s_t^i)))$. $\pi$ = actor, $V$ = critic.
Est. value with NN, not traj. rollouts.

\subsection{Motion synthesis}

\textbf{Data-driven:}
bad perf. out of distribution,
needs expensive mocap.

\textbf{DeepMimic:} RL to imitate reference motions while satisfying task objectives.

\textbf{SFV}: use pose estimation: videos $\to$ train data.

% \includesvg[pretex=\footnotesize, width=\linewidth]{figures/rl.svg}

\section{Neural Implicit Representations}

Voxels/volum. primitives are inefficient ($n^3$ compl.).
Meshes have limited granularity and have self-intersections.
\textbf{Implicit representation}: $S = \{x \mid f(x) = 0\}$.
Can be invertibly transformed without accuracy loss.
Usually represented as signed distance function values on a grid, but this is again $n^3$.
By UAT, approx. $f$ with NN.
\textbf{Occupancy networks}: predict probability that point is inside the shape.
\textbf{DeepSDF}: predict SDF.
Both conditioned on input (2D image, class, etc.).
Continuious, any topology/resolution, memory-efficient.
NFs can model other properties (color, force, etc.).

\subsection{Learning 3D Implicit Shapes}

\textbf{Inference}: to get a mesh, sample points, predict occupancy/SDF, use marching cubes.

\subsubsection{From watertight meshes}
Sample points in space, compute GT occupancy/SDF, CE loss.

\subsubsection{From point clouds} Only have samples on the surface.
Weak supervision: loss = $|f_\theta(x_i)|^2 + \lambda \E_x(\norm{\nabla_x f_\theta(x)} - 1)^2$,
edge points should have $\norm{\nabla f} \approx 1$ by def. of SDF, $f \approx 0$.

\subsubsection{From images} Need differentiable rendering 3D $\to$ 2D.
\textbf{Differentiable Volumetric Rendering}: for a point conditioned on encoded image,
predict occupancy $f(x)$ and RGB color $c(x)$.
\textbf{Forward}: for a pixel, raymarch and root find $\hat{p} : f(\hat{p}) = 0$ with secant. Set pixel color to $c(\hat{p})$.
\textbf{Backward}: see proofs.

\subsection{Neural Radiance Fields (NeRF)} \phantom{a}

$(x, y, z, \theta, \phi) \xrightarrow{\textsc{nn}} (r, g, b, \sigma)$.
Density is predicted before adding view direction $\theta, \phi$,
then one layer for color.
\textbf{Forward}: shoot ray, sample points along it and blend:
$\alpha \coloneqq 1 - \exp( - \sigma_i \delta_i), \delta_i \coloneqq t_{i+1} - t_i,
T_i \coloneqq \prod_{j = 1}^{i-1}(1 - \alpha_j)$,
color is $c = \sum_i T_i \alpha_i c_i$.
Optimized on many views of the scene.
Can handle transparency/thin structure,
but worse geometry.
Needs many (50+) views for training, slow rendering for high res,
only models static scenes.

\subsubsection{Positional Encoding for High Frequency Details}
Replace $x, y, z$ with pos. enc. or rand. Fourier feats.
Adds high frequency feats.

\subsubsection{NeRF from sparse views} Regularize geometry and color.

\subsubsection{Fast NeRF render. and train.}
Replace deep MLPs with learn. feature hash table + small MLP.
For $x$ interp. features between corners.

\subsection{3D Gaussian Splatting}

\textbf{Alternative parametr.}:
Find a cover of object with primitives, predict inside.
Or sphere clouds. Both ineff. for thin structures.
Ellipsoids are better.

Initialize point cloud randomly or with an approx. reconstruction.
Each point has a 3D Gaussian.
Use camera params. to project (``splat'') Gaussians to 2D
and differentiably render them.
Adaptive density control moves/clones/merges points.

Rasterization: for each pixel sort Gaussians by depth, opacity
$\alpha = o \cdot \exp( - 0.5(x - \mu')\tran \Sigma'{}^{-1}(x - \mu'))$,
rest same as NeRF.

Each Gaussian primitive has a center $\mu \in \R^3$, a covariance $\Sigma \in \R^{3\times 3}$, and a color $c \in \R^3$ and an opaciti $o \in \R$.

To model view-dependent color, the color can be replaced with spherical harmonics, i.e. $c \in \R^9$

To keep covariance semi-definite: $\Sigma = RSS^\top R^\top$, where $S \in \R^{3\times 3}$ is a diagonal scaling matrix and $R \in \R^{3\times 3}$ is a rotation matrix.

% \includesvg[width=\linewidth]{figures/splatting.svg}

\section{Parametric body models}

\subsection{Pictorial structure}

Unary terms and pairwise terms between them with springs.

\subsection{Deep features}

Direct regression: predict joint coordinates with refinement.

Heatmaps: predict probability for each pixel, maybe Gaussian.
Can do stages sequentially.

\subsection{3D}

Naive 2D $\to$ 3D lift works.
But can't define constraints $ \Rightarrow $ 2m arms sometimes.

\textbf{Skinned Multi-Person Linear model} (SMPL) is the standard non-commerical model.
3D mesh, base mesh is $\sim$7k vertices, designed by an artist.
To produce the model, point clouds (scans) need to be aligned with the mesh.
\textbf{Shape deformation subspace}: for a set of human meshes T-posing,
vectorize their vertices $T$ and subtract the mean mesh.
With PCA represent any person as weighted sum of 10-300 basis people, $T = S\beta + \mu$.

For pose, use \textbf{Linear Blend Skinning}.
$\vt_i' = \sum_k w_{ki} \mG_k(\bm\theta, \mJ)\vt_i$,
where $\vt$ is the T-pose positions of vertices,
$\vt'$ is transformed, $w$ are weights,
$\mG_k$ is rigid bone transf.,
$\bm\theta$ is pose, $\mJ$ are joint positions.
Linear assumption produces artifacts.
\textbf{SMPL}:
$\vt_i' = \sum_k w_{ki} \mG_k(\bm\theta, \mJ(\bm\beta))(\vt_i + \vs_i(\bm\beta) + \vp_i(\bm\theta))$.
Adds shape correctives $\vs(\bm\beta) = \mS\bm\beta$,
pose cor. $\vp(\bm\theta) = \mP\bm\theta$,
$\mJ$ dep. on shape $\bm\beta$.

Predicting human pose is just predicting $\bm\beta, \bm\theta$ and camera parameters.

\subsubsection{Optimization-based fitting} Predict 2D joint locations,
fit SMPL to them by argmin with prior regularization.
Argmin is hard to find, learn $F$: $\Delta \theta = F(\pdv{L_{reproj}}{\theta}, \theta^t, x)$.
Issues: self-occlusion, no depth info, non-rigid deformation (clothes).

\subsubsection{Template-based capture}
Scan for first frame, then track with SMPL.

\subsubsection{Animatable Neural Implicit Surfaces}

Model base shape and $w$ with 2 NISs.

\section{ML}

Perceptron converges in finite time iff data is linearly separable.
\textbf{MAP} $\theta^* \in \argmax p(\theta \mid X, y)$.
\textbf{MLE} $\theta \in \argmax p(y \mid X, \theta)$ consistent, efficient.
\textbf{Binary cross-entropy} $L(\theta) = - y_i \log(\hat{y}_i) - (1 - y_i) \log(1 - \hat{y}_i)$.
Cross-entropy $H(p_d, p_m) = H(p_d) + \kl{p_d}{p_m}$.
For any continuous $f$ $\exists \mathrm{NN}\ g(x), |g(x) - f(x)| < \varepsilon$.
1 hidden layer is enough, activation function needs to be nonlinear.

MLP backward inputs: $\delta^{(l)} = \delta^{(l + 1)} \cdot \pdv{\vz^{(l + 1)}}{\vz^{(l)}}$,
backward weights: $\left[\pdv{\vz^{(l)}}{\vw_{ij}^{(l)}}\right]_k = f'(\va)_k \cdot z_j^{(l)} \cdot [k = i],
\pdv{L}{\vz^{(l)}}{\vz^{(l)}} = \delta^{(l)} \pdv{\vz^{(l)}}{\vw_{i,j}^{(l)}}$,
backward bias: $\pdv{L}{\vb_i^{(l)}} =$ same, but no $\vz$.

\subsection{Activation functions} \phantom{a}

% \renewcommand{\arraystretch}{0}
\begin{tabular}[]{@{}c@{}ccc@{}}
    \toprule
    name & $f(x)$ & $f'(x)$ & $f(X)$ \\
    \midrule
    sigmoid & $\frac{1}{1 + e^{-x}}$ & $\sigma(x)(1 - \sigma(x))$ & $(0, 1)$ \\
    tanh & $\frac{e^x - e^{-x}}{e^x + e^{-x}}$ & $1 - \tanh(x)^2$ & $( - 1, 1)$ \\
    ReLU & $\max(0, x)$ & $[x \geq 0]$ & $[0, \infty)$ \\
\end{tabular}
Finite range: stable training, mapping to prob. space.
Sigmoid, tanh saturate (value with large mod have small gradient) $ \Rightarrow $ vanishing gradient,
Tanh is linear around 0 (easy learn),
ReLU can blow up activation; piecewise linear $ \Rightarrow $ faster convergence.

\subsection{GD algos}

\textbf{SGD}: use 1 sample.
For sum structured loss is unbiased.
High variance, efficient, jumps a lot $ \Rightarrow $ may get out of local min.,
may overshoot.
\textbf{Mini-batch}: use $m < n$ samples.
More stable, parallelized.
\textbf{Polyak's momentum}: velocity $\vv \coloneqq \alpha \vv - \epsilon \nabla_\theta L(\theta), \theta \coloneqq \theta + \vv$.
Move faster when high curv., consistent or noisy grad.
\textbf{Nesterov's momentum}: $\vv \coloneqq \alpha \vv - \epsilon \nabla_\theta L(\theta + \alpha \vv)$.
Gets grad. at future point.
\textbf{AdaGrad}: $\vr \coloneqq \vr + \nabla \odot \nabla$, $\Delta \theta = - \epsilon / (\delta + \sqrt{\vr})
\odot \nabla$.
Grads decrease fast for variables with high historical gradients,
slow for low. But can decrease LR too early/fast.
\textbf{RMSProp}: $\vr \coloneqq \rho \vr + (1 - \rho) \nabla \odot \nabla$,
use weighted moving average $ \Rightarrow $ drop history from distant past,
works better for nonconvex.
\textbf{Adam}: collect 1st and 2nd moments:
$\vm \coloneqq \beta_1 \vm + (1 - \beta_1) \nabla, \vv \coloneqq \beta_2 \vv + (1 - \beta_2) \nabla \odot \nabla$,
unbias:
$\hat{\vm} = \vm / (1 - \beta_1^t), \hat{\vv} = \vv / (1 - \beta_2^t)$,
$\Delta \theta =- \frac{\eta}{\sqrt{\hat{\vv}} + \epsilon}\hat{\vm}$.



\section{Proofs}

\subsection*{Softmax derivative}

Let $\hat{y}_i = f(x)i = \frac{\exp(x_i)}{\sum_{j = 1}^d \exp(x_j)}, x \in \R^d$,
$y$ is 1-hot $\in \R^d$,
negative log-likelihood
$L(\hat{y}, y) = - \sum_{i = 1}^d y_i \log \hat{y}_i$.

$\pdv{L}{x_i} = \pdv{L}{\hat{y}} \pdv{\hat{y}}{x_i}.\sep$
$\pdv{\hat{y}_i}{x_i} = \frac{\exp(x_i) \sum_{j = 1}^d \exp(x_j) - \exp^2(x_i)}{\left(\sum_{j = 1}^d \exp(x_j)\right)^2}
= \frac{\exp(x_i)}{\sum_{j = 1}^d \exp(x_j)} \left(\frac{\sum_{j = 1}^d \exp(x_j)}{\sum_{j = 1}^d \exp(x_j)} - \frac{\exp(x_i)}{\sum_{j = 1}^d \exp(x_j)}\right) = \hat{y}_i(1 - \hat{y}_i). \sep$
$\pdv{\hat{y}_k}{x_i} = \frac{ - \exp(x_i)\exp(x_k)}{\left(\sum_{j = 1}^d \exp(x_j)\right)^2} = - \hat{y}_i \hat{y}_k.\sep$
$\pdv{L}{\hat{y}_k} = - \frac{y_k}{\hat{y}_k}. \sep$
$\pdv{L}{x_i} = - \frac{y_i}{\hat{y}_i} (\hat{y}_i(1 - \hat{y}_i)) - \sum\limits_{k \neq i} \frac{y_k}{\hat{y}_k}( - \hat{y}_i \hat{y}_k) = - y_i + y_i \hat{y}_i + \sum\limits_{k \neq i} y_k \hat{y}_i
= - y_i + \hat{y}_i \sum_k y_k = \hat{y}_i - y_i$.

\subsection*{BPTT}

$\rho$ is the identity function, $\partial^+$ is the immediate derivative, ignoring the effect from recurrence.

$\pdv{\vh_t}{W} = \pdv{W} \vh_t(\rho(W), \vh_{t-1}(W))
= \pdv{\vh_t}{\rho} \pdv{\rho}{W} + \pdv{\vh_t}{\vh_{t-1}} \pdv{\vh_{t-1}}{W}
= \pdv{{}^+ \vh_t}{W} + \pdv{\vh_t}{\vh_{t-1}} \pdv{\vh_{t-1}}{W}
= \pdv{{}^+ \vh_t}{W} + \pdv{\vh_t}{\vh_{t-1}} \left[\pdv{{}^+ \vh_{t-1}}{W} + \pdv{\vh_{t-1}}{\vh_{t-2}}\pdv{{}^+ \vh_{t - 2}}{W} + \dots\right]
= \sum_{k = 1}^t \pdv{\vh_t}{\vh_k} \pdv{{}^+ \vh_k}{W} \sep \pdv{L_t}{W} = \pdv{L_t}{\hat{y}_t} \pdv{\hat{y}_t}{\vh_t} \sum_{k = 1}^t \pdv{\vh_t}{\vh_k} \pdv{{}^+ \vh_k}{W}$

\subsection*{BPTT divergence}

Let $\lambda_1$ be the largest singular value of $\mW_{hh}$,
$\norm{\diag f'(\vh_{i-1})} < \gamma, \gamma \in \R, \norm{\cdot}$ is the spectral norm.
If $\lambda_1 < \gamma^{-1}$, then
$\forall i \ \ \norm{\pdv{\vh_i}{\vh_{i-1}}} \leq \norm{\mW_{hh}\tran} \norm{\diag f'(\vh_{i-1})} < \frac{1}{\gamma}\gamma < 1$
$\Rightarrow \exists \eta : \forall i \ \ \norm{\pdv{\vh_i}{\vh_{i-1}}} \leq \eta < 1$,
by induction over $i$: $\norm{\prod_{i = k + 1}^t \pdv{\vh_i}{\vh_{i-1}}} \leq \eta^{t - k}$,
so the gradients vanish as $t \to \infty$.
Similarly, if $\lambda_1 > \gamma^{-1}$, then gradients explode.

\subsection*{$\kl{\cdot}{\cdot} \geq 0$}

$-\kl{p}{q} = -\E_{x \sim p} \log \frac{p(x)}{q(x)} = \E \log \frac{q(x)}{p(x)} \leq \log \E_{x \sim p} \frac{q(x)}{p(x)}
= \log \int q(x) \dd x = \log 1 = 0$.

\subsection*{VAE ELBO}

\def\ith{^{(i)}}
$\log p_\theta(x\ith)
= \E_{z \sim q_\phi(z \mid x\ith)} \log p_\theta(x\ith)
= \E_z \log \frac{p_\theta(x\ith \mid z) p_\theta(z)}{p_\theta(z \mid x\ith)}
= \E_z \log \frac{p_\theta(x\ith \mid z) p_\theta(z) q_\phi(z \mid x\ith)}{p_\theta(z \mid x\ith) q_\phi(z \mid x\ith)}
= \E_z \log p_\theta(x\ith \mid z) - \E_z \log \frac{q_\phi(z \mid x\ith)}{p_\theta(z)} + \E_z \log \frac{q_\phi(z \mid x\ith)}{p_\theta(z \mid x\ith)}
= \E_z \log p_\theta(x\ith \mid z) - \kl{q_\phi(z \mid x\ith)}{p_\theta(z)} + \kl{q_\phi(z \mid x\ith)}{p_\theta(z \mid x\ith)}$.

\subsection*{KL for ELBO} Let $p(z) = \mathcal{N}(0, \mI), q(z\mid x) = \mathcal{N}(\mu, \sigma^2 \mI)$, $J \coloneqq \dim z$.
By $\int p(z) \log q(z) \dd z = - \frac{J}{2} \log 2\pi - \frac{1}{2} \sum_{j = 1}^J \log\sigma^2_{q, j}
- \frac{1}{2} \sum_{j = 1}^J \frac{\sigma_{p, j}^2 + (\mu_{p, j} - \mu_{q, j})^2}{\sigma^2_{q, j}}$
we have $\int q(z\mid x) \log p(z) \dd z = - \frac{J}{2} \log 2\pi - \frac{1}{2} \sum_{j = 1}^J(\sigma_j^2 + \mu_j^2)$
and $\int q(z\mid x) \log q(z\mid x) \dd z = \frac{J}{2} \log 2\pi - \frac{1}{2} \sum_{j = 1}^J(\log \sigma_j^2 + 1)$,
so $ - \kl{q(z\mid x)}{p(z)} = \frac{1}{2} \sum_{j = 1}^J (1 + \log \sigma_j^2 - \mu_j^2 - \sigma_j^2)$

\subsection*{Optimal discriminator} $D^*$ maximizes $V(G, D) = \int_x p_d \log D(x) \dd x + \int_z p(z) \log(1 - D(G(Z))) \dd z
= \int_x p_d \log D(x) \dd x + p_m(x) \log(1 - D(x)) \dd z$,
and for $f(y) = a\log(y) + b\log(1 - y):$
$f'(y) = \frac{a}{y} - \frac{b}{1 - y} \Rightarrow f'(y) = 0 \Leftrightarrow y = \frac{a}{a + b}$,
$f''(\frac{a}{a + b}) = - \frac{a}{\left(\frac{a}{a + b}\right)^2} - \frac{b}{\left(1 - \frac{a}{a + b}\right)^2} < 0$
for $a, b > 0 \Rightarrow $ max. at $\frac{a}{a + b}$ $\Rightarrow$ $D^* = \frac{p_d(x)}{p_d(x) + p_m(x)}$

\subsection*{Expectation of reparam.}

\!\!$\nabla_\varphi \E_{p_\varphi(z)}(f(z)) = \nabla_\varphi \int p_\varphi(z) f(z) \dd z
= \nabla_\varphi\! \int\!\! p_\varphi(z) f(z) \dd z \!=\! \nabla_\varphi\! \int\!\! p_\varphi(z) f(g(\epsilon, \varphi)) \dd \epsilon
\!=\! \E_{p(\epsilon)}\! \nabla_\varphi f(g(\epsilon, \varphi))$

\subsection*{$q(x_t \mid x_0)$}
$\vx_t = \sqrt{1 - \beta_t} \vx_{t-1} + \sqrt{\beta_t} \epsilon
= \sqrt{\alpha_t} \vx_{t-1} + \sqrt{1 - \alpha_t} \epsilon
= \sqrt{\alpha_t \alpha_{t - 1}} \vx_{t-2} + \sqrt{1 - \alpha_t \alpha_{t - 1}} \epsilon
= \dots = \sqrt{\overline{\alpha}_t} \vx_0 + \sqrt{1 - \overline{\alpha}_t} \epsilon$

\subsection*{Bellman operator converges} Want to prove that value iteration converges to the optimal policy:
$\lim_{k \to \infty} (T^*)^k(V) = V_*$,
where $T^*(V) = \max_{a \in A} \sum_{s', r} p(s', r \mid s, a)(r(s, a) + \gamma V(s'))$.
$T^*$ is a contraction mapping, i.e. $\max_{s \in S}|T^*(V_1(s)) - T^*(V_2(s))| \leq \gamma\max_{s \in S}|V_1(s) - V_2(s)|$:
$\mathrm{LHS} \leq \max_{s, a}|\sum_{s', r} p(s', r \mid s, a)(r(s, a) + \gamma V_1(s')) - \sum_{s', r} p(s', r \mid s, a)(r(s, a) + \gamma V_2(s'))|
= \gamma \max_{s, a}|\sum_{s', r} p(s', r \mid s, a)(V_1(s') - V_2(s')| = \mathrm{RHS}$.
By the contraction th., $T^*$ has a unique fixed point, and we know $V^*$ is a FP of $T^*$.
As $\gamma < 1$, $\mathrm{LHS}(V, V^*) \to 0$ and $T^*(V) \to V_*$.

\subsection*{Policy gradients}
\quad$J(\theta) = \E_{\tau \sim p(\tau)} [r(\tau)]
= \int p(\tau) r(\tau) \dd \tau$.
$\nabla_\theta J(\theta) = \int \nabla_\theta p(\tau) r(\tau) \dd \tau
= \int p(\tau) \nabla_\theta \log p(\tau) r(\tau) \dd \tau
= \E_{\tau \sim p(\tau)} [\nabla_\theta \log p(\tau) r(\tau)] = \E_{\tau \sim p(\tau)}[\nabla_\theta \log p(\tau) r(\tau)]$.

$\log p(\tau) = \log[p(s_1)\prod \pi_\theta(a_t \mid s_t) p(s_{t+1} \mid a_t, s_t)]
= 0 + \sum_t \log \pi_\theta(a_t \mid s_t) + 0$

$\nabla_\theta J(\theta) = \E_{\tau \sim p(\tau)}
[\textcolor{blue}{(\sum_t \nabla \log p_\theta(a_t^i \mid s_t^i)})\textcolor{purple}{(\sum_t \gamma^t r(s_t^i, a_t^i))}]$:
\textcolor{blue}{max likelihood}, \textcolor{purple}{trajectory reward} scales the gradient.

\subsection*{Implicit differentiation}

$\dv{y}{x}$ of $x^2 + y^2 = 1$:

$\dv{x}(x^2 + y^2) = \dv{x}(1) \Rightarrow \dv{x}x^2 + \dv{x}y^2 = 0 \Rightarrow 2x + (\dv{y} y^2) \dv{y}{x} = 0$
$\Rightarrow 2x + 2y \dv{y}{x} = 0 \Rightarrow \dv{y}{x} =- \frac{x}{y}$

\subsection*{DVR Backward pass}
$\pdv{L}{\theta} = \sum_u \pdv{L}{\hat\mI_u} \cdot \pdv{\hat\mI_u}{\theta} \sep\!$
$\pdv{\hat\mI_u}{\theta} = \pdv{c_\theta(\hat\vp)}{\theta} + \pdv{t_\theta(\hat\vp)}{\hat\vp}
 \cdot \pdv{\hat\vp}{\theta}$.
Ray $\hat\vp = r_0 + \hat{d}\vw$,
$r_0$ is camera pos., $\vw$ is ray dir., $\hat{d}$ is ray dist.
Implicit def.: $f_\theta(\hat\vp) = \tau$.
Diff.:
$\pdv{f_\theta(\hat\vp)}{\theta} + \pdv{f_\theta(\hat\vp)}{\hat\vp} \cdot \pdv{\hat\vp}{\theta} = 0 \Rightarrow
\pdv{f_\theta(\hat\vp)}{\theta} + \pdv{f_\theta(\hat\vp)}{\hat\vp} \cdot \vw\pdv{\hat{d}}{\theta} = 0 \Rightarrow 
\pdv{\hat\vp}{\theta} = \vw \pdv{\hat{d}}{\theta} = - \vw(\pdv{f_\theta(\hat{\vp})}{\hat{\vp}} \cdot \vw)^{-1}
\pdv{f_\theta(\hat\vp)}{\theta}$

\section{Appendix}

\subsection*{Secant Method}

Line $(x_0, f(x_0)) \to (x_1, f(x_1))$, approx.:
$y = \frac{f(x_1) - f(x_0)}{x_1 - x_0}(x - x_1) + f(x_1)$,
$y = 0$ at $x_2 = x_1 - f(x_1) \frac{x_1 - x_0}{f(x_1) - f(x_0)}$.
Approximates Newton's method without derivatives.

\subsection*{Implicit plane from 3 points}

$(x_1, 0, 0), (0, y_1, 0), (0, 0, z_1) \Rightarrow x / x_1 + y / y_1 + z / z_1 - 1 = 0$.
More generally: let $a, b$ any vectors on plane,
$n \coloneqq a \times b = (a_2 b_3 - a_3 b_2, a_3b_1 - a_1b_3, a_1b_2 - a_2b_1) \Rightarrow n_1 x + n_2 y + n_3 z + k = 0$,
subst. any point to find $k$.

\subsection*{Torus equation}

\!\!\!\!$(\sqrt{x^2 + y^2} - R)^2 + z^2 = r^2$, cent. $0$, around $z$ axis.

\subsection*{Chain rule}
$\pdv{z_k}{x_j} = \sum_{i=1}^{m} \pdv{z_k}{y_i} \pdv{y_i}{x_j} \sep$
$\pdv{z}{x} = \pdv{z}{y} \pdv{y}{x}$


\subsection*{Derivatives}

$(f \cdot g)' = f'g + fg'$,
$(f / g)' = (f'g - fg') / g^2$,
$(f \circ g)' = f'(g)g'$,
$(f^{-1})' = 1 / f'(f^{-1})$,
$(\log x)' = 1 / x$.

\subsection*{Linear algebra}

$\det(\mA + \vu\vv\tran) = (1 + \vv\tran \mA^{-1}\vu) \det \mA$

\subsection*{Jensen's inequality}
$f(\E[X]) \leq \E[f(X)]$ and $f(tx_1 + (1 - t)x_2) \leq tf(x_1) + (1 - t)f(x_2)$ if $f$ is convex, i.e. $\forall t \in [0, 1], x_1, x_2 \in X : f(tx_1 + (1 - t)x_2) \leq tf(x_1) + (1 - t)f(x_2)$.

\subsection*{Gaussians}

$\mathcal{N}(\mu_1, \Sigma_1) + \mathcal{N}(\mu_2, \Sigma_2)
= \mathcal{N}(\mu_1 + \mu_2, \Sigma_1 + \Sigma_2)$,

$a \cdot \mathcal{N}(\mu, \Sigma) = \mathcal{N}(a\mu, a^2\Sigma)$.

$\mathcal{N} = \frac{1}{\sqrt{(2\pi)^d |\Sigma|}} \exp( - \frac{1}{2} (x - \mu)\tran \Sigma^{-1} (x - \mu))$

\subsection*{VRNN}

$p_\theta(\vz) = \prod_{t = 1}^T p_\theta(z_t \mid \vz_{ < t}, \vx_{ < t})$,

$q_\phi(\vx \mid \vz) = \prod_{t = 1}^T q_\phi(z_t \mid \vx_{ \leq t}, \vz_{ < t})$,

$p_\theta(\vx, \vz) = \prod_{t = 1}^T p_\theta(x_t \mid \vz_{ \leq t}, \vz_{ < t})p_\theta(z_t \mid \vz_{ < t}, \vx_{ < t})$.

\subsection*{Misc}
A \textbf{translation vector} is added.

\textbf{Bayes rule}:
$P(A \mid B) = P(B \mid A)P(A) / P(B)$.

A function $f$ is \textbf{volume preserving} if $\left|\det \pdv{f^{-1}(x)}{x}\right| = 1$.

\textbf{Negative log-likelihood} $L(\hat{y}, y) = - \sum_i y_i \log \hat{y}_i$

\end{multicols*}

\end{document}
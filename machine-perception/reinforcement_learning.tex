\section{Reinforcement learning}

Environment is a Markov Decision Process:
states $S$, actions $A$,
reward $r : S \times A \to \R$,
transition $p: S \times A \to S$,
initial $s_0 \in S$,
discount factor $\gamma$.
$r$ and $p$ are deterministic, can be a distribution.
Learn policy $\pi : S \to A$.
Value $V_\pi: S \to \R$, the reward from $s$ under $\pi$.
\textbf{Bellman eq.}: $G_t \coloneqq \sum_{k = 0}^\infty \gamma^k R_{t + k + 1},$
$v_\pi(s) \coloneqq \E_\pi[G_t \mid S_t = s]
= \E_\pi[R_{t+1} + \gamma G_{t+1} \mid S_t = s]
= \sum_a \pi(a \mid s) \sum_{s'} \sum_r p(s', r \mid s, a)[r + \gamma \E_\pi[G_{t+1} \mid S_{t+1} = s']]
= \sum_a \pi(a \mid s) \sum_{s', r} p(s', r \mid s, a)[r + \gamma v_\pi(s')]]$.
Can be solved via dynamic programming (needs knowledge of $p$),
Monte-Carlo or Temporal Difference learning.

\subsection{Dynamic programming}

Value iteration: compute optimal $v_*$, then $\pi_*$.

Policy iteration: compute $v_\pi$ and $\pi$ together.

For any $V_\pi$ the greedy policy (optimal) is
$\pi'(s) = \argmax_{a \in A}(r(s, a) + \gamma V_\gamma(p(s, a)))$.

\textbf{Bellman optimality}: $v_*(s) = \max_a q_*(s, a)
= \max_a \sum_{s', r} p(s', r \mid s, a)[r + \gamma v_*(s')]$
$ \Rightarrow $ update step:
$V_{\mathrm{new}}^*(s) = \max_{a \in A} (r(s, a) + \gamma V_{\mathrm{old}}^*(s'))$,
when $V_{\mathrm{old}}^* = V_{\mathrm{new}}^*$, we have optimal policy.

Converges in finite steps, more efficient than policy iteration.
But needs knowledge of $p$,
iterates over all states and $\mathcal{O}(|S|)$ memory.

\subsection{Monte Carlo sampling}

Sample trajectories, estimate $v_\pi$ by averaging returns.
Doesn't need full $p$, is unbiased,
but high variance, exploration/exploitation dilemma, may not reach term. state.

\subsection{Temporal Difference learning}

For each $s \to s'$ by action $a$ update:
$\Delta V(s) = r(s, a) + \gamma V(s') - V(s)$.
\textbf{$\bm\varepsilon$-greedy policy}: with prob. $\varepsilon$ choose random action, else greedy.

\subsection{Q-learning}

$Q$-value f.: $q_\pi(s, a) \coloneqq \E_\pi[G_t \mid S_t = s, A_t = a]$.

\textbf{SARSA} (on-policy):
For each $S \to S'$ by action $A$ update:
$\Delta Q(S, A) = r(S, A) + \gamma Q(S', A') - Q(S, A)$,
$Q(S, A) {}+{}\!\!\!= \alpha \Delta Q(S, A)$,
$\alpha$ is LR.

\textbf{Q-learning} (off-policy/offline):
$\Delta Q(S, A) = R_{t+1} + \gamma \max_a Q(S', a) - Q(S, A)$

All these approaches do not approximate values of states that have not been visited.

\subsection{Deep Q-learning}

Use NN to predict $Q$-values.
Loss is $(R + \gamma \max_{a'}Q_\theta(S', a') - Q_\theta(S, A))^2$,
backprop only through $Q_\theta(S, A)$.
Store history in replay buffer, sample from it for training $ \Rightarrow $ no correlation in samples.

\subsection{Deep Q-networks}

Encode state to low dimensionality with NN.

\subsection{Policy gradients}

$Q$-learning does not handle continuous action spaces.
Learn a policy directly instead,
$\pi(a_t \mid s_t) = \mathcal{N}(\mu_t, \sigma_t^2 \mid s_t)$.
Sample trajectories: $p(\tau) = p(s_1, a_1, \dots, s_T, a_T) = p(s_1) \prod \pi(a_t \!\!\mid\!\! s_t) p(s_{t+1} \!\!\mid\!\! a_t, s_t)$.
This is on-policy.

Eval: $J(\theta) \coloneqq \E_{\tau \sim p_\theta(\tau)} [\sum_t \gamma^t r(s_t, a_t)]$.
To optimize, need to compute $\E$ (see proofs).

\textbf{REINFORCE}: MC sampling of $\tau$.
To reduce variance, subtract baseline $b(s_t)$ from reward.

\subsection{Actor-Critic}

$\nabla_\theta J(\theta) = \frac{1}{N} \sum_i \sum_t \nabla \log \pi_\theta(a_t^i \mid s_t^i)(r(s_t^i, a_t^i) + \gamma V(s_{t+1}^i) - V(s_t^i)))$. $\pi$ = actor, $V$ = critic.
Est. value with NN, not traj. rollouts.

\subsection{Motion synthesis}

\textbf{Data-driven:}
bad perf. out of distribution,
needs expensive mocap.

\textbf{DeepMimic:} RL to imitate reference motions while satisfying task objectives.

\textbf{SFV}: use pose estimation: videos $\to$ train data.

\includesvg[pretex=\footnotesize, width=\linewidth]{figures/rl.svg}

\section{Appendix}
\subsection*{Bellman operator converges} Want to prove that value iteration converges to the optimal policy:
$\lim_{k \to \infty} (T^*)^k(V) = V_*$,
where $T^*(V) = \max_{a \in A} \sum_{s', r} p(s', r \mid s, a)(r(s, a) + \gamma V(s'))$.
$T^*$ is a contraction mapping, i.e. $\max_{s \in S}|T^*(V_1(s)) - T^*(V_2(s))| \leq \gamma\max_{s \in S}|V_1(s) - V_2(s)|$:
$\mathrm{LHS} \leq \max_{s, a}|\sum_{s', r} p(s', r \mid s, a)(r(s, a) + \gamma V_1(s')) - \sum_{s', r} p(s', r \mid s, a)(r(s, a) + \gamma V_2(s'))|
= \gamma \max_{s, a}|\sum_{s', r} p(s', r \mid s, a)(V_1(s') - V_2(s')| = \mathrm{RHS}$.
By the contraction th., $T^*$ has a unique fixed point, and we know $V^*$ is a FP of $T^*$.
As $\gamma < 1$, $\mathrm{LHS}(V, V^*) \to 0$ and $T^*(V) \to V_*$.


\subsection*{Policy gradients}
\quad$J(\theta) = \E_{\tau \sim p(\tau)} [r(\tau)]
= \int p(\tau) r(\tau) \dd \tau$.
$\nabla_\theta J(\theta) = \int \nabla_\theta p(\tau) r(\tau) \dd \tau
= \int p(\tau) \nabla_\theta \log p(\tau) r(\tau) \dd \tau
= \E_{\tau \sim p(\tau)} [\nabla_\theta \log p(\tau) r(\tau)] = \E_{\tau \sim p(\tau)}[\nabla_\theta \log p(\tau) r(\tau)]$.

$\log p(\tau) = \log[p(s_1)\prod \pi_\theta(a_t \mid s_t) p(s_{t+1} \mid a_t, s_t)]
= 0 + \sum_t \log \pi_\theta(a_t \mid s_t) + 0$

$\nabla_\theta J(\theta) = \E_{\tau \sim p(\tau)}
[\textcolor{blue}{(\sum_t \nabla \log p_\theta(a_t^i \mid s_t^i)})\textcolor{purple}{(\sum_t \gamma^t r(s_t^i, a_t^i))}]$:
\textcolor{blue}{max likelihood}, \textcolor{purple}{trajectory reward} scales the gradient.
